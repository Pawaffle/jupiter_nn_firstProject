<html>
<head>
<title>Fine-tuned_VGG16-final.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #bcbec4;}
.s1 { color: #cf8e6d;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #2aacb8;}
.s5 { color: #7a7e85;}
.ls0 { height: 1px; border-width: 0; color: #43454a; background-color:#43454a}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
Fine-tuned_VGG16-final.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
# Fine-tuned VGG16 model <hr class="ls0">#%% 
</span><span class="s1">import </span><span class="s0">os</span>
<span class="s0">os</span><span class="s2">.</span><span class="s0">environ</span><span class="s2">[</span><span class="s3">'TF_ENABLE_ONEDNN_OPTS'</span><span class="s2">] = </span><span class="s3">'0'</span>

<span class="s1">import </span><span class="s0">shutil</span><span class="s2">, </span><span class="s0">pathlib</span>
<span class="s1">import </span><span class="s0">keras</span>
<span class="s1">import </span><span class="s0">matplotlib</span><span class="s2">.</span><span class="s0">pyplot </span><span class="s1">as </span><span class="s0">plt</span>
<span class="s1">import </span><span class="s0">numpy </span><span class="s1">as </span><span class="s0">np</span>
<span class="s1">from </span><span class="s0">keras </span><span class="s1">import </span><span class="s0">layers</span>
<span class="s1">from </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">utils </span><span class="s1">import </span><span class="s0">image_dataset_from_directory</span>

<span class="s0">new_base_dir </span><span class="s2">= </span><span class="s0">pathlib</span><span class="s2">.</span><span class="s0">Path</span><span class="s2">(</span><span class="s3">&quot;./NewSet&quot;</span><span class="s2">)</span><hr class="ls0"><span class="s0">#%% md 
Tässä luomme train-, validation- ja test-datasetit suoraan hakemistorakenteesta. Jokainen kuva rescalataan yhtenäiseen kokoon (224×224), batching hoituu automaattisesti, ja lopuksi varmistetaan että kaikki neljä classia löytyy kaikista dataseteistä. <hr class="ls0">#%% 
batch_size </span><span class="s2">= </span><span class="s4">32</span>
<span class="s0">img_size </span><span class="s2">= (</span><span class="s4">224</span><span class="s2">, </span><span class="s4">224</span><span class="s2">)</span>

<span class="s0">train_dataset </span><span class="s2">= </span><span class="s0">image_dataset_from_directory</span><span class="s2">(</span>
    <span class="s0">new_base_dir </span><span class="s2">/ </span><span class="s3">&quot;train&quot;</span><span class="s2">,</span>
    <span class="s0">image_size</span><span class="s2">=</span><span class="s0">img_size</span><span class="s2">,</span>
    <span class="s0">batch_size</span><span class="s2">=</span><span class="s0">batch_size</span>
<span class="s2">)</span>

<span class="s0">validation_dataset </span><span class="s2">= </span><span class="s0">image_dataset_from_directory</span><span class="s2">(</span>
    <span class="s0">new_base_dir </span><span class="s2">/ </span><span class="s3">&quot;validation&quot;</span><span class="s2">,</span>
    <span class="s0">image_size</span><span class="s2">=</span><span class="s0">img_size</span><span class="s2">,</span>
    <span class="s0">batch_size</span><span class="s2">=</span><span class="s0">batch_size</span>
<span class="s2">)</span>

<span class="s0">test_dataset </span><span class="s2">= </span><span class="s0">image_dataset_from_directory</span><span class="s2">(</span>
    <span class="s0">new_base_dir </span><span class="s2">/ </span><span class="s3">&quot;test&quot;</span><span class="s2">,</span>
    <span class="s0">image_size</span><span class="s2">=</span><span class="s0">img_size</span><span class="s2">,</span>
    <span class="s0">batch_size</span><span class="s2">=</span><span class="s0">batch_size</span>
<span class="s2">)</span>

<span class="s5"># Make sure all 4 classes are present in all 3 datasets</span>
<span class="s0">print</span><span class="s2">(</span><span class="s0">train_dataset</span><span class="s2">.</span><span class="s0">class_names</span><span class="s2">, </span><span class="s0">validation_dataset</span><span class="s2">.</span><span class="s0">class_names</span><span class="s2">, </span><span class="s0">test_dataset</span><span class="s2">.</span><span class="s0">class_names</span><span class="s2">)</span><hr class="ls0"><span class="s0">#%% md 
Tässä otetaan käyttöön ImageNetillä pre-trained VGG16 (pelkkä conv-base ilman top layeria), ja laitetaan koko runko freeze-tilaan, jotta se toimii fixed feature extractorina ennen varsinaista fine-tuning -vaihetta. <hr class="ls0">#%% 
conv_base </span><span class="s2">= </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">applications</span><span class="s2">.</span><span class="s0">VGG16</span><span class="s2">(</span>
    <span class="s0">weights</span><span class="s2">=</span><span class="s3">&quot;imagenet&quot;</span><span class="s2">,</span>
    <span class="s0">include_top</span><span class="s2">=</span><span class="s1">False</span><span class="s2">,</span>
    <span class="s0">input_shape</span><span class="s2">=(</span><span class="s4">224</span><span class="s2">, </span><span class="s4">224</span><span class="s2">, </span><span class="s4">3</span><span class="s2">)</span>
<span class="s2">)</span>

<span class="s5"># Ensin: jäädytä kaikki</span>
<span class="s0">conv_base</span><span class="s2">.</span><span class="s0">trainable </span><span class="s2">= </span><span class="s1">False</span>

<span class="s0">conv_base</span><span class="s2">.</span><span class="s0">summary</span><span class="s2">()</span><hr class="ls0"><span class="s0">#%% md 
Tässä määritellään yksinkertainen data augmentation -setti, joka tekee kuville satunnaista horizontal flip -kääntöä, pientä rotationia ja zoomausta, jotta malli yleistyy paremmin ilman että datasettiä tarvitsee oikeasti kasvattaa. <hr class="ls0">#%% 
data_augmentation </span><span class="s2">= </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">Sequential</span><span class="s2">([</span>
    <span class="s0">layers</span><span class="s2">.</span><span class="s0">RandomFlip</span><span class="s2">(</span><span class="s3">&quot;horizontal&quot;</span><span class="s2">),</span>
    <span class="s0">layers</span><span class="s2">.</span><span class="s0">RandomRotation</span><span class="s2">(</span><span class="s4">0.1</span><span class="s2">),</span>
    <span class="s0">layers</span><span class="s2">.</span><span class="s0">RandomZoom</span><span class="s2">(</span><span class="s4">0.2</span><span class="s2">)</span>
<span class="s2">])</span><hr class="ls0"><span class="s0">#%% md 
Tässä luodaan end-to-end-malli, jossa kuvat kulkevat ensin data augmentationin ja preprocessingin läpi, sitten VGG16:n conv-baseen, ja lopuksi oman Dense-pään kautta neljän classin softmax-prediktioon. <hr class="ls0">#%% 
inputs </span><span class="s2">= </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">Input</span><span class="s2">(</span><span class="s0">shape</span><span class="s2">=(</span><span class="s4">224</span><span class="s2">, </span><span class="s4">224</span><span class="s2">, </span><span class="s4">3</span><span class="s2">))</span>
<span class="s0">x </span><span class="s2">= </span><span class="s0">data_augmentation</span><span class="s2">(</span><span class="s0">inputs</span><span class="s2">)</span>
<span class="s0">x </span><span class="s2">= </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">applications</span><span class="s2">.</span><span class="s0">vgg16</span><span class="s2">.</span><span class="s0">preprocess_input</span><span class="s2">(</span><span class="s0">x</span><span class="s2">)</span>
<span class="s0">x </span><span class="s2">= </span><span class="s0">conv_base</span><span class="s2">(</span><span class="s0">x</span><span class="s2">)</span>
<span class="s0">x </span><span class="s2">= </span><span class="s0">layers</span><span class="s2">.</span><span class="s0">Flatten</span><span class="s2">()(</span><span class="s0">x</span><span class="s2">)</span>
<span class="s0">x </span><span class="s2">= </span><span class="s0">layers</span><span class="s2">.</span><span class="s0">Dense</span><span class="s2">(</span><span class="s4">256</span><span class="s2">, </span><span class="s0">activation</span><span class="s2">=</span><span class="s3">&quot;relu&quot;</span><span class="s2">)(</span><span class="s0">x</span><span class="s2">)</span>
<span class="s0">x </span><span class="s2">= </span><span class="s0">layers</span><span class="s2">.</span><span class="s0">Dropout</span><span class="s2">(</span><span class="s4">0.5</span><span class="s2">)(</span><span class="s0">x</span><span class="s2">)</span>
<span class="s0">outputs </span><span class="s2">= </span><span class="s0">layers</span><span class="s2">.</span><span class="s0">Dense</span><span class="s2">(</span><span class="s4">4</span><span class="s2">, </span><span class="s0">activation</span><span class="s2">=</span><span class="s3">&quot;softmax&quot;</span><span class="s2">)(</span><span class="s0">x</span><span class="s2">)</span>

<span class="s0">model </span><span class="s2">= </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">Model</span><span class="s2">(</span><span class="s0">inputs</span><span class="s2">, </span><span class="s0">outputs</span><span class="s2">)</span>
<span class="s0">model</span><span class="s2">.</span><span class="s0">summary</span><span class="s2">()</span><hr class="ls0"><span class="s0">#%% md 
Tässä model compile -rivillä asetetaan lossiksi sparse_categorical_crossentropy, optimizeriksi RMSprop pienellä learning ratella, ja callback huolehtii siitä että talletetaan aina paras (val_lossin perusteella) versio mallista. <hr class="ls0">#%% 
model</span><span class="s2">.</span><span class="s0">compile</span><span class="s2">(</span>
    <span class="s0">loss</span><span class="s2">=</span><span class="s3">&quot;sparse_categorical_crossentropy&quot;</span><span class="s2">,</span>
    <span class="s0">optimizer</span><span class="s2">=</span><span class="s0">keras</span><span class="s2">.</span><span class="s0">optimizers</span><span class="s2">.</span><span class="s0">RMSprop</span><span class="s2">(</span><span class="s0">learning_rate</span><span class="s2">=</span><span class="s4">1e-4</span><span class="s2">),</span>
    <span class="s0">metrics</span><span class="s2">=[</span><span class="s3">&quot;accuracy&quot;</span><span class="s2">]</span>
<span class="s2">)</span>

<span class="s0">callbacks </span><span class="s2">= [</span>
    <span class="s0">keras</span><span class="s2">.</span><span class="s0">callbacks</span><span class="s2">.</span><span class="s0">ModelCheckpoint</span><span class="s2">(</span>
        <span class="s0">filepath</span><span class="s2">=</span><span class="s3">&quot;fine-tuned.keras&quot;</span><span class="s2">,</span>
        <span class="s0">save_best_only</span><span class="s2">=</span><span class="s1">True</span><span class="s2">,</span>
        <span class="s0">monitor</span><span class="s2">=</span><span class="s3">&quot;val_loss&quot;</span><span class="s2">)</span>
<span class="s2">]</span><hr class="ls0"><span class="s0">#%% md 
Tässä ajetaan ensimmäinen training-vaihe, jossa vain modelin oma Dense-pää oppii, kun taas VGG16:n runko on vielä kokonaan freeze-tilassa. Tavoitteena saada hyvä perusluokitin ennen varsinaista fine-tuningia. <hr class="ls0">#%% 
history </span><span class="s2">= </span><span class="s0">model</span><span class="s2">.</span><span class="s0">fit</span><span class="s2">(</span>
    <span class="s0">train_dataset</span><span class="s2">,</span>
    <span class="s0">epochs</span><span class="s2">=</span><span class="s4">10</span><span class="s2">,</span>
    <span class="s0">validation_data</span><span class="s2">=</span><span class="s0">validation_dataset</span><span class="s2">,</span>
    <span class="s0">callbacks</span><span class="s2">=</span><span class="s0">callbacks</span>
<span class="s2">)</span><hr class="ls0"><span class="s0">#%% 
</span><span class="s5"># Ladataan paras malli tämän vaiheen jälkeen</span>
<span class="s0">model </span><span class="s2">= </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">models</span><span class="s2">.</span><span class="s0">load_model</span><span class="s2">(</span><span class="s3">&quot;fine-tuned.keras&quot;</span><span class="s2">)</span><hr class="ls0"><span class="s0">#%% 
</span><span class="s5"># Testataan tässä vaiheessa</span>
<span class="s0">test_loss</span><span class="s2">, </span><span class="s0">test_acc </span><span class="s2">= </span><span class="s0">model</span><span class="s2">.</span><span class="s0">evaluate</span><span class="s2">(</span><span class="s0">test_dataset</span><span class="s2">)</span>
<span class="s0">print</span><span class="s2">(</span><span class="s3">f&quot;Vaihe 1 – test accuracy: </span><span class="s1">{</span><span class="s0">test_acc</span><span class="s1">:</span><span class="s3">.3f</span><span class="s1">}</span><span class="s3">&quot;</span><span class="s2">)</span><hr class="ls0"><span class="s0">#%% md 
Tässä laitetaan conv_base trainableksi, mutta pidetään kaikki paitsi 4 viimeistä layeria edelleen freeze-tilassa, compiletaan malli pienemmällä learning ratella ja ajetaan toinen training-phase, jossa tehdään varsinainen fine-tuning koko pipelineen. <hr class="ls0">#%% 
</span><span class="s5"># 1. Get the layer from *inside* the model</span>
<span class="s0">base_model_layer </span><span class="s2">= </span><span class="s0">model</span><span class="s2">.</span><span class="s0">layers</span><span class="s2">[</span><span class="s4">2</span><span class="s2">]</span>

<span class="s5"># 2. Unfreeze *that* layer</span>
<span class="s0">base_model_layer</span><span class="s2">.</span><span class="s0">trainable </span><span class="s2">= </span><span class="s1">True</span>

<span class="s5"># 3. Loop through its internal layers</span>
<span class="s1">for </span><span class="s0">layer </span><span class="s1">in </span><span class="s0">base_model_layer</span><span class="s2">.</span><span class="s0">layers</span><span class="s2">[:-</span><span class="s4">4</span><span class="s2">]:</span>
    <span class="s0">layer</span><span class="s2">.</span><span class="s0">trainable </span><span class="s2">= </span><span class="s1">False</span>

<span class="s0">model</span><span class="s2">.</span><span class="s0">compile</span><span class="s2">(</span>
    <span class="s0">loss</span><span class="s2">=</span><span class="s3">&quot;sparse_categorical_crossentropy&quot;</span><span class="s2">,</span>
    <span class="s0">optimizer</span><span class="s2">=</span><span class="s0">keras</span><span class="s2">.</span><span class="s0">optimizers</span><span class="s2">.</span><span class="s0">RMSprop</span><span class="s2">(</span><span class="s0">learning_rate</span><span class="s2">=</span><span class="s4">1e-5</span><span class="s2">),</span>
    <span class="s0">metrics</span><span class="s2">=[</span><span class="s3">&quot;accuracy&quot;</span><span class="s2">]</span>
<span class="s2">)</span>

<span class="s0">model</span><span class="s2">.</span><span class="s0">summary</span><span class="s2">()</span><hr class="ls0"><span class="s0">#%% 
callbacks_finetune </span><span class="s2">= [</span>
    <span class="s0">keras</span><span class="s2">.</span><span class="s0">callbacks</span><span class="s2">.</span><span class="s0">ModelCheckpoint</span><span class="s2">(</span>
        <span class="s0">filepath</span><span class="s2">=</span><span class="s3">&quot;fine_tuned_final.keras&quot;</span><span class="s2">,</span>
        <span class="s0">save_best_only</span><span class="s2">=</span><span class="s1">True</span><span class="s2">,</span>
        <span class="s0">monitor</span><span class="s2">=</span><span class="s3">&quot;val_loss&quot;</span>
    <span class="s2">)</span>
<span class="s2">]</span>

<span class="s0">history_finetune </span><span class="s2">= </span><span class="s0">model</span><span class="s2">.</span><span class="s0">fit</span><span class="s2">(</span>
    <span class="s0">train_dataset</span><span class="s2">,</span>
    <span class="s0">epochs</span><span class="s2">=</span><span class="s4">20</span><span class="s2">,</span>
    <span class="s0">validation_data</span><span class="s2">=</span><span class="s0">validation_dataset</span><span class="s2">,</span>
    <span class="s0">callbacks</span><span class="s2">=</span><span class="s0">callbacks_finetune</span>
<span class="s2">)</span><hr class="ls0"><span class="s0">#%% md 
Tässä yhdistetään ensimmäisen ja fine-tuning-vaiheen accuracy- ja loss-historiat, ja piirretään niistä training/validation-käyrät, jotta nähdään koko oppimisprosessi yhdellä silmäyksellä. <hr class="ls0">#%% 
</span><span class="s5"># kerätään historiat yhteen, piirtäen kummatkin:</span>
<span class="s0">acc </span><span class="s2">= </span><span class="s0">history</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;accuracy&quot;</span><span class="s2">] + </span><span class="s0">history_finetune</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;accuracy&quot;</span><span class="s2">]</span>
<span class="s0">val_acc </span><span class="s2">= </span><span class="s0">history</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;val_accuracy&quot;</span><span class="s2">] + </span><span class="s0">history_finetune</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;val_accuracy&quot;</span><span class="s2">]</span>
<span class="s0">loss </span><span class="s2">= </span><span class="s0">history</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;loss&quot;</span><span class="s2">] + </span><span class="s0">history_finetune</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;loss&quot;</span><span class="s2">]</span>
<span class="s0">val_loss </span><span class="s2">= </span><span class="s0">history</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;val_loss&quot;</span><span class="s2">] + </span><span class="s0">history_finetune</span><span class="s2">.</span><span class="s0">history</span><span class="s2">[</span><span class="s3">&quot;val_loss&quot;</span><span class="s2">]</span>

<span class="s0">epochs </span><span class="s2">= </span><span class="s0">range</span><span class="s2">(</span><span class="s4">1</span><span class="s2">, </span><span class="s0">len</span><span class="s2">(</span><span class="s0">acc</span><span class="s2">) + </span><span class="s4">1</span><span class="s2">)</span>

<span class="s0">plt</span><span class="s2">.</span><span class="s0">figure</span><span class="s2">()</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">plot</span><span class="s2">(</span><span class="s0">epochs</span><span class="s2">, </span><span class="s0">acc</span><span class="s2">, </span><span class="s0">label</span><span class="s2">=</span><span class="s3">&quot;Training accuracy&quot;</span><span class="s2">)</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">plot</span><span class="s2">(</span><span class="s0">epochs</span><span class="s2">, </span><span class="s0">val_acc</span><span class="s2">, </span><span class="s0">label</span><span class="s2">=</span><span class="s3">&quot;Validation accuracy&quot;</span><span class="s2">)</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">legend</span><span class="s2">()</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">title</span><span class="s2">(</span><span class="s3">&quot;Accuracy&quot;</span><span class="s2">)</span>

<span class="s0">plt</span><span class="s2">.</span><span class="s0">figure</span><span class="s2">()</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">plot</span><span class="s2">(</span><span class="s0">epochs</span><span class="s2">, </span><span class="s0">loss</span><span class="s2">, </span><span class="s0">label</span><span class="s2">=</span><span class="s3">&quot;Training loss&quot;</span><span class="s2">)</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">plot</span><span class="s2">(</span><span class="s0">epochs</span><span class="s2">, </span><span class="s0">val_loss</span><span class="s2">, </span><span class="s0">label</span><span class="s2">=</span><span class="s3">&quot;Validation loss&quot;</span><span class="s2">)</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">legend</span><span class="s2">()</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">title</span><span class="s2">(</span><span class="s3">&quot;Loss&quot;</span><span class="s2">)</span>
<span class="s0">plt</span><span class="s2">.</span><span class="s0">show</span><span class="s2">()</span><hr class="ls0"><span class="s0">#%% 
</span><span class="s1">import </span><span class="s0">numpy </span><span class="s1">as </span><span class="s0">np</span>
<span class="s1">import </span><span class="s0">matplotlib</span><span class="s2">.</span><span class="s0">pyplot </span><span class="s1">as </span><span class="s0">plt</span>

<span class="s5"># 1. Collect all test data and labels</span>
<span class="s0">x_test </span><span class="s2">= []</span>
<span class="s0">y_true </span><span class="s2">= []</span>

<span class="s1">for </span><span class="s0">images</span><span class="s2">, </span><span class="s0">labels </span><span class="s1">in </span><span class="s0">test_dataset</span><span class="s2">:</span>
    <span class="s0">x_test</span><span class="s2">.</span><span class="s0">append</span><span class="s2">(</span><span class="s0">images</span><span class="s2">.</span><span class="s0">numpy</span><span class="s2">())</span>
    <span class="s0">y_true</span><span class="s2">.</span><span class="s0">append</span><span class="s2">(</span><span class="s0">labels</span><span class="s2">.</span><span class="s0">numpy</span><span class="s2">())</span>

<span class="s0">x_test </span><span class="s2">= </span><span class="s0">np</span><span class="s2">.</span><span class="s0">concatenate</span><span class="s2">(</span><span class="s0">x_test</span><span class="s2">, </span><span class="s0">axis</span><span class="s2">=</span><span class="s4">0</span><span class="s2">)</span>
<span class="s0">y_true </span><span class="s2">= </span><span class="s0">np</span><span class="s2">.</span><span class="s0">concatenate</span><span class="s2">(</span><span class="s0">y_true</span><span class="s2">, </span><span class="s0">axis</span><span class="s2">=</span><span class="s4">0</span><span class="s2">)</span>

<span class="s5"># 2. Get predictions</span>
<span class="s0">y_pred_probs </span><span class="s2">= </span><span class="s0">model</span><span class="s2">.</span><span class="s0">predict</span><span class="s2">(</span><span class="s0">x_test</span><span class="s2">)</span>
<span class="s0">y_pred_classes </span><span class="s2">= </span><span class="s0">np</span><span class="s2">.</span><span class="s0">argmax</span><span class="s2">(</span><span class="s0">y_pred_probs</span><span class="s2">, </span><span class="s0">axis</span><span class="s2">=</span><span class="s4">1</span><span class="s2">)</span>
<span class="s0">class_names </span><span class="s2">= </span><span class="s0">test_dataset</span><span class="s2">.</span><span class="s0">class_names</span>

<span class="s5"># 3. Find ONLY mistakes</span>
<span class="s0">mistake_indices </span><span class="s2">= </span><span class="s0">np</span><span class="s2">.</span><span class="s0">where</span><span class="s2">(</span><span class="s0">y_pred_classes </span><span class="s2">!= </span><span class="s0">y_true</span><span class="s2">)[</span><span class="s4">0</span><span class="s2">]</span>

<span class="s1">if </span><span class="s0">len</span><span class="s2">(</span><span class="s0">mistake_indices</span><span class="s2">) == </span><span class="s4">0</span><span class="s2">:</span>
    <span class="s0">print</span><span class="s2">(</span><span class="s3">&quot;</span><span class="s1">\n</span><span class="s3">SUCCESS: The model made 0 mistakes! (Accuracy is 100%)&quot;</span><span class="s2">)</span>
    <span class="s0">print</span><span class="s2">(</span><span class="s3">&quot;There are no incorrect images to display.&quot;</span><span class="s2">)</span>
<span class="s1">else</span><span class="s2">:</span>
    <span class="s0">print</span><span class="s2">(</span><span class="s3">f&quot;</span><span class="s1">\n</span><span class="s3">Found </span><span class="s1">{</span><span class="s0">len</span><span class="s2">(</span><span class="s0">mistake_indices</span><span class="s2">)</span><span class="s1">} </span><span class="s3">mistakes.&quot;</span><span class="s2">)</span>

    <span class="s5"># Show only the mistakes</span>
    <span class="s1">for </span><span class="s0">k </span><span class="s1">in </span><span class="s0">mistake_indices</span><span class="s2">:</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">figure</span><span class="s2">(</span><span class="s0">figsize</span><span class="s2">=(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">3</span><span class="s2">))</span>

        <span class="s5"># Image</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">subplot</span><span class="s2">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">)</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">imshow</span><span class="s2">(</span><span class="s0">x_test</span><span class="s2">[</span><span class="s0">k</span><span class="s2">].</span><span class="s0">astype</span><span class="s2">(</span><span class="s3">&quot;uint8&quot;</span><span class="s2">))</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">axis</span><span class="s2">(</span><span class="s3">'off'</span><span class="s2">)</span>

        <span class="s5"># Bar chart of probabilities</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">subplot</span><span class="s2">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">)</span>
        <span class="s0">probs </span><span class="s2">= </span><span class="s0">y_pred_probs</span><span class="s2">[</span><span class="s0">k</span><span class="s2">]</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">bar</span><span class="s2">(</span><span class="s0">np</span><span class="s2">.</span><span class="s0">arange</span><span class="s2">(</span><span class="s0">len</span><span class="s2">(</span><span class="s0">class_names</span><span class="s2">)), </span><span class="s0">probs</span><span class="s2">)</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">xticks</span><span class="s2">(</span><span class="s0">np</span><span class="s2">.</span><span class="s0">arange</span><span class="s2">(</span><span class="s0">len</span><span class="s2">(</span><span class="s0">class_names</span><span class="s2">)), </span><span class="s0">class_names</span><span class="s2">, </span><span class="s0">rotation</span><span class="s2">=</span><span class="s4">45</span><span class="s2">)</span>

        <span class="s0">pred_label </span><span class="s2">= </span><span class="s0">class_names</span><span class="s2">[</span><span class="s0">y_pred_classes</span><span class="s2">[</span><span class="s0">k</span><span class="s2">]]</span>
        <span class="s0">true_label </span><span class="s2">= </span><span class="s0">class_names</span><span class="s2">[</span><span class="s0">y_true</span><span class="s2">[</span><span class="s0">k</span><span class="s2">]]</span>

        <span class="s5"># Title is always RED for mistakes</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">ylabel</span><span class="s2">(</span><span class="s3">&quot;Probability&quot;</span><span class="s2">)</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">title</span><span class="s2">(</span><span class="s3">f&quot;Predicted: </span><span class="s1">{</span><span class="s0">pred_label</span><span class="s1">} </span><span class="s3">| True: </span><span class="s1">{</span><span class="s0">true_label</span><span class="s1">}</span><span class="s3">&quot;</span><span class="s2">, </span><span class="s0">color</span><span class="s2">=</span><span class="s3">'red'</span><span class="s2">, </span><span class="s0">fontweight</span><span class="s2">=</span><span class="s3">'bold'</span><span class="s2">)</span>

        <span class="s0">plt</span><span class="s2">.</span><span class="s0">tight_layout</span><span class="s2">()</span>
        <span class="s0">plt</span><span class="s2">.</span><span class="s0">show</span><span class="s2">()</span><hr class="ls0"><span class="s0">#%% md 
Lopullinen testaus <hr class="ls0">#%% 
best_model </span><span class="s2">= </span><span class="s0">keras</span><span class="s2">.</span><span class="s0">models</span><span class="s2">.</span><span class="s0">load_model</span><span class="s2">(</span><span class="s3">&quot;fine_tuned_final.keras&quot;</span><span class="s2">)</span>
<span class="s0">test_loss</span><span class="s2">, </span><span class="s0">test_acc </span><span class="s2">= </span><span class="s0">best_model</span><span class="s2">.</span><span class="s0">evaluate</span><span class="s2">(</span><span class="s0">test_dataset</span><span class="s2">)</span>
<span class="s0">print</span><span class="s2">(</span><span class="s3">f&quot;Lopullinen test accuracy (fine-tuned): </span><span class="s1">{</span><span class="s0">test_acc</span><span class="s1">:</span><span class="s3">.3f</span><span class="s1">}</span><span class="s3">&quot;</span><span class="s2">)</span><hr class="ls0"><span class="s0">#%% md 
## Fine-tuned mallin vertaaminen muihin malleihin (CNN from scrath ja VGG16 feature extraction) <hr class="ls0">#%% md 
### Malli 1 – CNN from scratch (CNN_trees) 
 
- Rakennettu alusta asti itse (Conv2D + MaxPooling + Dense). 
 
- Treenaus suoraan kuva-dataseteillä + data augmentation. 
 
- Test accuracy ≈ 0.90 (90 %). 
 
### Malli 2 – VGG16 feature extraction (Feature_Extraction_VGG16) 
 
- Esikoulutettu VGG16 (ImageNet), conv-osa jäädytetty. 
 
- Ajetaan kuvat kerran läpi conv-basen → tallennetaan train_features, val_features, test_features. 
 
- Niiden päälle koulutetaan vain tiheä luokittelupää (Flatten + Dense + Dropout + Dense(4, softmax)). 
 
- Test accuracy ≈ 0.99 (99 %). 
 
### Malli 3 – Fine-tuned VGG16 (Fine-tuned_VGG16) 
 
- Sama VGG16-pohja kuin mallissa 2, mutta: 
 
    1. Ensin vaihe, jossa VGG16 on jäädytetty + data augmentation + luokittelupää → erittäin hyvä perusmalli. 
 
    2. Sitten hienosäätö: avataan VGG16:n viimeiset 4 kerrosta trainableksi, pienempi learning rate, jatkokoulutus koko pipelinea. 
 
    3. Lopuksi paras fine-tunattu malli tallennetaan fine_tuned_final.keras ja testataan. 
 
- Lopullinen test accuracy ≈ 1.00 (100 %). <hr class="ls0">#%% md 
Fine-tunattu VGG16 on sekä numeerisesti paras (100 % testissä) että käyttäytyy ”terveimmin” validaatiodatan suhteen (stabiili ja pieni val_loss). CNN from scratch jää selvästi jälkeen, ja pelkkä feature extraction -malli näyttää hieman epävakaammalta (val_loss-ongelma), vaikka accuracy on korkea. <hr class="ls0">#%% md 
Fine-tuned VGG16 -malli ylitti selvästi kaksi aiempaa mallia. Alusta asti koulutettu CNN (malli 1) saavutti noin 90 % testitarkkuuden, mutta sen validointimittarit olivat melko epävakaita, mikä kertoo rajallisesta kapasiteetista ja datan määrästä. Pelkkä VGG16-featureiden käyttö (malli 2) nosti tarkkuuden hyvin korkeaksi, mutta validointihäviö kasvoi ajoittain merkittävästi, mikä viittaa luokittelupään ylisovittamiseen precompute-piirteisiin. Hienosäädetty VGG16 (malli 3) yhdisti esikoulutetun verkon edut ja domain-kohtaisen hienosäädön: vain verkon ylimmät kerrokset avattiin opetettaviksi pienellä oppimisnopeudella. Tämä johti tasaisesti matalaan validointihäviöön ja 100 %:n testitarkkuuteen, eli parhaan generalisaatiosuorituskyvyn saavutti nimenomaan fine-tunattu esikoulutettu malli.</span></pre>
</body>
</html>